"""sparse_datasets_experiment.py - åªè·‘ rcv1 å’Œ news20"""
import sys, os
sys.path.insert(0, os.path.abspath('./src'))

from pathlib import Path
import numpy as np
import csv
from datetime import datetime
from typing import List, Tuple

from capymoa.stream import LibsvmStream
from capymoa.classifier import FTRLClassifier


# ==================== é…ç½® ====================
BASE_DIR = Path("C:/reposity clone/OpenMOA/experiments/FTRL_experiment")
DATA_DIR = BASE_DIR / "data"
RESULTS_DIR = BASE_DIR / "results"
RESULTS_DIR.mkdir(exist_ok=True, parents=True)

# ç¨€ç–æ•°æ®é›†é…ç½®
SPARSE_CONFIG = {
    'rcv1': {
        'file': DATA_DIR / 'rcv1.binary',
        'alpha_grid': [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20],  # è®ºæ–‡ç”¨çš„æ›´å°çš„alpha
        'lambda': 0.05,  # å…ˆç”¨ 0.05 è¯•è¯•
    },
    'news20': {
        'file': DATA_DIR / 'news20.binary',
        'alpha_grid': [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20],
        'lambda': 0.05,
    },
}

TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")


class SparseExperimentResults:
    """å­˜å‚¨ç¨€ç–æ•°æ®é›†å®éªŒç»“æœ"""
    def __init__(self, dataset_name: str):
        self.dataset_name = dataset_name
        self.best_alpha = None
        self.grid_search_results = []
        self.final_accuracy = None
        self.final_sparsity = None
        self.final_density = None
        self.num_instances = None
        self.num_features = None


def grid_search_sparse(
    dataset_name: str,
    file_path: Path,
    alpha_grid: List[float],
    lambda_l1: float
) -> Tuple[float, List[Tuple[float, float, float]]]:
    """åœ¨ç¨€ç–æ•°æ®é›†ä¸Šè¿›è¡Œgrid search"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ” Grid Search: {dataset_name.upper()}")
    print(f"{'='*70}")
    
    # åˆ›å»ºstreamï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼Œé¿å…é‡å¤æ‰«æï¼‰
    stream = LibsvmStream(
        path=file_path,
        dataset_name=dataset_name,
        target_type="categorical"
    )
    
    print(f"\n{'Alpha':<10} {'Accuracy':<12} {'Sparsity':<12} {'Time(s)':<10} {'Status'}")
    print("-"*70)
    
    best_alpha = None
    best_acc = -1
    grid_results = []
    
    import time
    
    for alpha in alpha_grid:
        t0 = time.time()
        
        learner = FTRLClassifier(
            schema=stream.get_schema(),
            alpha=alpha,
            beta=1.0,
            l1=lambda_l1,
            l2=1.0
        )
        
        correct = 0
        stream.restart()
        
        for instance in stream:
            if learner.predict(instance) == instance.y_index:
                correct += 1
            learner.train(instance)
        
        t_elapsed = time.time() - t0
        
        acc = correct / len(stream) * 100
        sparsity = learner.get_sparsity()
        grid_results.append((alpha, acc, sparsity))
        
        status = ""
        if acc > best_acc:
            best_acc = acc
            best_alpha = alpha
            status = "â­ NEW BEST"
        
        print(f"{alpha:<10.2f} {acc:<12.3f} {sparsity:<12.3f} {t_elapsed:<10.1f} {status}")
    
    print(f"\nâœ… Best alpha: {best_alpha} (Accuracy: {best_acc:.3f}%)")
    
    return best_alpha, grid_results


def evaluate_sparse_final(
    dataset_name: str,
    file_path: Path,
    alpha: float,
    lambda_l1: float
) -> SparseExperimentResults:
    """ç”¨æœ€ä¼˜alphaè¿›è¡Œæœ€ç»ˆè¯„ä¼°"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Final Evaluation: {dataset_name.upper()}")
    print(f"{'='*70}")
    print(f"Using Î± = {alpha}, Î» = {lambda_l1}")
    
    import time
    t0 = time.time()
    
    stream = LibsvmStream(
        path=file_path,
        dataset_name=dataset_name,
        target_type="categorical"
    )
    
    learner = FTRLClassifier(
        schema=stream.get_schema(),
        alpha=alpha,
        beta=1.0,
        l1=lambda_l1,
        l2=1.0
    )
    
    correct = 0
    total = 0
    
    print("\nTraining progress:")
    for i, instance in enumerate(stream, 1):
        if learner.predict(instance) == instance.y_index:
            correct += 1
        learner.train(instance)
        total += 1
        
        # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯5000ä¸ªå®ä¾‹ï¼‰
        if i % 5000 == 0:
            elapsed = time.time() - t0
            print(f"  Processed {i}/{len(stream)} instances ({i/len(stream)*100:.1f}%) - {elapsed:.1f}s")
    
    t_total = time.time() - t0
    
    acc = correct / total * 100
    sparsity = learner.get_sparsity()
    density = 1.0 - sparsity
    
    # æ„å»ºç»“æœå¯¹è±¡
    result = SparseExperimentResults(dataset_name)
    result.best_alpha = alpha
    result.final_accuracy = acc
    result.final_sparsity = sparsity
    result.final_density = density
    result.num_instances = total
    result.num_features = learner.get_num_total_features()
    
    print(f"\n{'='*70}")
    print(f"Results:")
    print(f"  Accuracy:       {acc:.3f}%")
    print(f"  Sparsity:       {sparsity:.3f} ({sparsity*100:.1f}%)")
    print(f"  Density:        {density:.3f} ({density*100:.1f}%)")
    print(f"  Total features: {result.num_features:,}")
    print(f"  Non-zero:       {learner.get_num_active_features():,}")
    print(f"  Zero weights:   {learner.get_num_zero_weights():,}")
    print(f"  Training time:  {t_total:.1f}s")
    print(f"  Paper format:   {acc:.1f} ({density:.3f})")
    
    return result


def save_sparse_results(
    results: List[SparseExperimentResults],
    results_dir: Path,
    timestamp: str
):
    """ä¿å­˜ç¨€ç–æ•°æ®é›†å®éªŒç»“æœ"""
    
    # 1. ä¿å­˜æ±‡æ€»TXT
    summary_file = results_dir / f"sparse_datasets_results_{timestamp}.txt"
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("FTRL-Proximal on Sparse Datasets\n")
        f.write("rcv1.binary and news20.binary\n")
        f.write("="*80 + "\n\n")
        
        f.write("Implementation:\n")
        f.write("  - Sparse FTRL with lazy weight computation\n")
        f.write("  - LibsvmStream for LIBSVM format\n")
        f.write("  - Single run (as per paper)\n\n")
        
        f.write("Parameters:\n")
        f.write("  Î» (L1):      0.05\n")
        f.write("  Î² (beta):    1.0\n")
        f.write("  L2:          1.0\n\n")
        
        # ç»“æœå¯¹æ¯”
        f.write("="*80 + "\n")
        f.write("RESULTS COMPARISON\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"{'Dataset':<15} {'Your Result':<30} {'Paper Result':<30}\n")
        f.write("-"*80 + "\n")
        
        # è®ºæ–‡ç»“æœï¼ˆæ ¹æ®è®ºæ–‡ Table 2ï¼‰
        paper_results = {
            'rcv1': (95.5, 0.032),  # å‡è®¾å€¼ï¼Œéœ€è¦æŸ¥è®ºæ–‡
            'news20': (88.9, 0.054),  # å‡è®¾å€¼ï¼Œéœ€è¦æŸ¥è®ºæ–‡
        }
        
        for result in results:
            paper_acc, paper_density = paper_results.get(result.dataset_name, (0, 0))
            your_format = f"{result.final_accuracy:.1f} ({result.final_density:.3f})"
            paper_format = f"{paper_acc:.1f} ({paper_density:.3f})" if paper_acc > 0 else "N/A"
            
            f.write(f"{result.dataset_name:<15} {your_format:<30} {paper_format}\n")
        
        # è¯¦ç»†ä¿¡æ¯
        f.write("\n" + "="*80 + "\n")
        f.write("DETAILED INFORMATION\n")
        f.write("="*80 + "\n\n")
        
        for result in results:
            f.write(f"{result.dataset_name.upper()}:\n")
            f.write(f"  Best Î±:         {result.best_alpha}\n")
            f.write(f"  Accuracy:       {result.final_accuracy:.3f}%\n")
            f.write(f"  Sparsity:       {result.final_sparsity:.3f}\n")
            f.write(f"  Density:        {result.final_density:.3f}\n")
            f.write(f"  Instances:      {result.num_instances:,}\n")
            f.write(f"  Total features: {result.num_features:,}\n\n")
    
    print(f"\nğŸ’¾ Summary saved: {summary_file}")
    
    # 2. ä¿å­˜CSV
    csv_file = results_dir / f"sparse_datasets_results_{timestamp}.csv"
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        writer.writerow(['Dataset', 'Best Alpha', 'Accuracy', 'Sparsity', 'Density', 
                        'Instances', 'Total Features'])
        
        for result in results:
            writer.writerow([
                result.dataset_name,
                result.best_alpha,
                f"{result.final_accuracy:.3f}",
                f"{result.final_sparsity:.3f}",
                f"{result.final_density:.3f}",
                result.num_instances,
                result.num_features
            ])
    
    print(f"ğŸ’¾ CSV saved: {csv_file}")


def run_sparse_experiment():
    """è¿è¡Œç¨€ç–æ•°æ®é›†å®éªŒ"""
    
    print("\n" + "â•”" + "="*78 + "â•—")
    print("â•‘" + " "*15 + "FTRL on Sparse Datasets (rcv1, news20)" + " "*24 + "â•‘")
    print("â•š" + "="*78 + "â•")
    
    print(f"\nğŸ“ Data directory: {DATA_DIR}")
    print(f"ğŸ“ Results directory: {RESULTS_DIR}")
    
    all_results = []
    
    for dataset_name, config in SPARSE_CONFIG.items():
        file_path = config['file']
        
        if not file_path.exists():
            print(f"\nâš ï¸  Skipping {dataset_name}: file not found")
            print(f"     Expected: {file_path}")
            continue
        
        print(f"\n\n{'#'*80}")
        print(f"# Processing: {dataset_name.upper()}")
        print(f"{'#'*80}")
        
        try:
            # Step 1: Grid search
            best_alpha, grid_results = grid_search_sparse(
                dataset_name=dataset_name,
                file_path=file_path,
                alpha_grid=config['alpha_grid'],
                lambda_l1=config['lambda']
            )
            
            # Step 2: æœ€ç»ˆè¯„ä¼°
            result = evaluate_sparse_final(
                dataset_name=dataset_name,
                file_path=file_path,
                alpha=best_alpha,
                lambda_l1=config['lambda']
            )
            
            result.grid_search_results = grid_results
            all_results.append(result)
            
        except Exception as e:
            print(f"\nâŒ Error processing {dataset_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # ==================== ä¿å­˜ç»“æœ ====================
    if all_results:
        print("\n\n" + "="*80)
        print("SAVING RESULTS")
        print("="*80)
        
        save_sparse_results(all_results, RESULTS_DIR, TIMESTAMP)
        
        # æ‰“å°æœ€ç»ˆæ±‡æ€»
        print("\n\n" + "="*80)
        print("FINAL SUMMARY")
        print("="*80 + "\n")
        
        print(f"{'Dataset':<15} {'Accuracy':<15} {'Density':<15} {'Features':<15}")
        print("-"*70)
        for r in all_results:
            print(f"{r.dataset_name:<15} {r.final_accuracy:<15.3f} {r.final_density:<15.3f} {r.num_features:<15,}")
        
        print("\n" + "="*80)
        print("âœ… Sparse datasets experiment completed!")
        print(f"ğŸ“‚ Results saved in: {RESULTS_DIR}")
        
    else:
        print("\nâŒ No results to save.")
        print("\nğŸ’¡ æç¤ºï¼š")
        print("  1. ç¡®ä¿ rcv1.binary å’Œ news20.binary æ–‡ä»¶å­˜åœ¨")
        print("  2. æ–‡ä»¶åº”è¯¥åœ¨: C:/reposity clone/OpenMOA/experiments/FTRL_experiment/data/")
        print("  3. æ–‡ä»¶æ ¼å¼: LIBSVMæ ¼å¼ (label feature_id:value ...)")


if __name__ == "__main__":
    try:
        run_sparse_experiment()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Experiment interrupted by user.")
    except Exception as e:
        print(f"\n\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()